{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Using-extreme-gradient-boosting-to-detect-glottal-closure-instants-in-speech-signal\" data-toc-modified-id=\"Using-extreme-gradient-boosting-to-detect-glottal-closure-instants-in-speech-signal-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Using extreme gradient boosting to detect glottal closure instants in speech signal</a></div><div class=\"lev2 toc-item\"><a href=\"#Training-and-evaluating-the-classifier-on-UWB-data\" data-toc-modified-id=\"Training-and-evaluating-the-classifier-on-UWB-data-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Training and evaluating the classifier on UWB data</a></div><div class=\"lev2 toc-item\"><a href=\"#CMU-data\" data-toc-modified-id=\"CMU-data-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>CMU data</a></div><div class=\"lev2 toc-item\"><a href=\"#GCI-detection-evaluation\" data-toc-modified-id=\"GCI-detection-evaluation-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>GCI detection evaluation</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# A COMPARISON OF CONVOLUTIONAL NEURAL NETWORKS FOR GLOTTAL CLOSURE INSTANT DETECTION FROM RAW SPEECH\n",
    "\n",
    "This is an example of a Python code to train and test an InceptionV3-1D model, a deep one-dimensional convolutional neural network (CNN), for detecting glottal closure instants (GCIs) in the speech signal. See the [corresponding paper](paper/matousek_ICASSP2021_paper.pdf) for more details.\n",
    "\n",
    "[Keras](https://keras.io/) (v2.3.1) with [TensorFlow](https://www.tensorflow.org/) (v1.15.3) backend are used to train and evaluate the CNN model.\n",
    "\n",
    "Prerequisities are stored in the [requirements](requirements.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we make imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import random as pyrandom\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as skm\n",
    "import utils\n",
    "from inception1D import InceptionV31D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the training and evaluation of the InceptionV3-1D model, we describe data firstly. Note that just a [sample of data](data/sample) will be used in this tutorial (40 waveforms for training and 2 waveforms for testing from 2 voice talents). In the [corresponding paper](paper/matousek_ICASSP2021_paper.pdf), 3200 waveforms from 16 voice talents were used.\n",
    "\n",
    "The following sample of data is used:\n",
    "* `spc8 ...` speech waveforms downsampled to 8 kHz\n",
    "* `negpeaks ...` indeces of negative peaks in the (filtered) speech waveform\n",
    "* `targets ...` ground truth GCIs associated with the negative peaks (1=GCI, 0=non-GCI)\n",
    "\n",
    "We used the [Multi-Phase Algorithm](http://www.sciencedirect.com/science/article/pii/S0167639311000094) (MPA) to detect GCIs from the contemporaneous electroglottograph (EGG) signal and used the detected GCIs as the ground truth ones.\n",
    "\n",
    "As can be seen, the number of GCIs and non-GCIs in our data is heavily unbalanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# peaks:    10990\n",
      "# GCI:      8659\n",
      "# non-GCI:  2331\n"
     ]
    }
   ],
   "source": [
    "utt_list = np.loadtxt('data/sample/train.txt', 'str')\n",
    "targets = np.hstack([np.load(osp.join('data/sample/targets', u+'.npy')) for u in utt_list])\n",
    "\n",
    "print('# peaks:   ', len(targets))\n",
    "print('# GCI:     ', len(targets[targets > 0]))\n",
    "print('# non-GCI: ', len(targets[targets == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is caused by the 8kHz sampling as there are fewer peaks in unvoiced segments taken as non-GCIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Training and evaluating the CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code sets the randomness and tries to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 7\n",
    "# Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "# Set python built-in pseudo-random generator at a fixed value\n",
    "pyrandom.seed(seed_value)\n",
    "# Set numpy pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "# Set the tensorflow pseudo-random generator at a fixed value\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "# Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we read train/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "X_trn, y_trn, input_shape = utils.load_data('data/sample/train.txt', 'data/sample/spc8', 'data/sample/negpeaks',\n",
    "                                            'data/sample/targets', frame_length=0.03, winfunc=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and check the shape of inputted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (240, 1)\n",
      "# of training examples: 10990\n",
      "# of samples per frame: 240\n"
     ]
    }
   ],
   "source": [
    "print('Input shape: ', input_shape)\n",
    "print('# of training examples:', X_trn.shape[0])\n",
    "print('# of samples per frame:', X_trn.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val, input_shape = utils.load_data('data/sample/val.txt', 'data/sample/spc8', 'data/sample/negpeaks',\n",
    "                                            'data/sample/targets', frame_length=0.03, winfunc=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of validation examples: 457\n"
     ]
    }
   ],
   "source": [
    "print('# of validation examples:', X_val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "In this example, we use 1D version of the InceptionV3 model which is shown in the paper to achieve best results on the test set. The definition of the model is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = InceptionV31D(input_shape)\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can train the model on the train set and evaluate it on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 10990 samples, validate on 457 samples\n",
      "Epoch 1/2\n",
      "10990/10990 [==============================] - 294s 27ms/step - loss: 0.1354 - accuracy: 0.9495 - val_loss: 0.4859 - val_accuracy: 0.8249\n",
      "Epoch 2/2\n",
      "10990/10990 [==============================] - 274s 25ms/step - loss: 0.1002 - accuracy: 0.9599 - val_loss: 0.6024 - val_accuracy: 0.8271\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_trn, y_trn, validation_data=(X_val, y_val), epochs=2, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "In this very simplified example, the accuracy on the validation set was about 83%. Much better results can be obtained when more training data from more voice talents is used, when tuning of the hyper-parameters (such as the frame size, batch size, learning rate, etc.) is done and also when the model is trained for more epochs. Please see the [paper](paper/Matousek_ICASSP2021_paper.pdf) for more details.\n",
    "\n",
    "Since the data is unbalanced, the _accuracy_ score could be confusing. In the [paper](paper/Matousek_ICASSP2021_paper.pdf), we use $F1$, _recall_ ($R$), and _precision_ ($P$) scores. For this purpose, we firstly get the prediction of each peak to be GCI or non_GCI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457/457 [==============================] - 4s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict to get some other metrics\n",
    "y_proba = model.predict(X_val, verbose=1)[:, 0]\n",
    "y_pred = utils.proba2classes(y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we use [Scikit-learn](http://scikit-learn.org/stable/) tools to calculate the measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 90.52%\n",
      "R  = 100.00%\n",
      "P  = 82.68%\n"
     ]
    }
   ],
   "source": [
    "print('F1 = {:.2%}'.format(skm.f1_score(y_val, y_pred)))\n",
    "print('R  = {:.2%}'.format(skm.recall_score(y_val, y_pred)))\n",
    "print('P  = {:.2%}'.format(skm.precision_score(y_val, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
