{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# A COMPARISON OF CONVOLUTIONAL NEURAL NETWORKS FOR GLOTTAL CLOSURE INSTANT DETECTION FROM RAW SPEECH\n",
    "\n",
    "This is an example of a Python code to train and test an InceptionV3-1D model, a deep one-dimensional convolutional neural network (CNN), for detecting glottal closure instants (GCIs) in the speech signal. See the [corresponding paper](paper/matousek_ICASSP2021_paper.pdf) for more details.\n",
    "\n",
    "[Keras](https://keras.io/) (v2.3.1) with [TensorFlow](https://www.tensorflow.org/) (v1.15.3) backend are used to train and evaluate the CNN model.\n",
    "\n",
    "Prerequisities are stored in the [requirements](requirements.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we make import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import random as pyrandom\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as skm\n",
    "from keras.models import model_from_json\n",
    "import librosa as lr\n",
    "import utils\n",
    "import wav_manip as wm\n",
    "import gci_utils as gu\n",
    "from inception1D import InceptionV31D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the training and evaluation of the InceptionV3-1D model, we describe data firstly. Note that just a [sample of data](data/sample) will be used in this tutorial (40 waveforms for training and 2 waveforms for testing from 2 voice talents). In the [corresponding paper](paper/matousek_ICASSP2021_paper.pdf), 3200 waveforms from 16 voice talents were used.\n",
    "\n",
    "The following sample of data is used:\n",
    "* `spc8 ...` speech waveforms downsampled to 8 kHz\n",
    "* `negpeaks ...` indeces of negative peaks in the (filtered) speech waveform\n",
    "* `targets ...` ground truth GCIs associated with the negative peaks (1=GCI, 0=non-GCI)\n",
    "\n",
    "We used the [Multi-Phase Algorithm](http://www.sciencedirect.com/science/article/pii/S0167639311000094) (MPA) to detect GCIs from the contemporaneous electroglottograph (EGG) signal and used the detected GCIs as the ground truth ones.\n",
    "\n",
    "As can be seen, the number of GCIs and non-GCIs in our data is heavily unbalanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# peaks:    10990\n",
      "# GCIs:     8659\n",
      "# non-GCIs: 2331\n"
     ]
    }
   ],
   "source": [
    "utt_list = np.loadtxt('data/sample/train.txt', 'str')\n",
    "targets = np.hstack([np.load(osp.join('data/sample/targets', u+'.npy')) for u in utt_list])\n",
    "\n",
    "print('# peaks:   ', len(targets))\n",
    "print('# GCIs:    ', len(targets[targets > 0]))\n",
    "print('# non-GCIs:', len(targets[targets == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is caused by the 8kHz sampling as there are fewer peaks in unvoiced segments taken as non-GCIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Training and evaluating the CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code sets the randomness and tries to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 7\n",
    "# Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "# Set python built-in pseudo-random generator at a fixed value\n",
    "pyrandom.seed(seed_value)\n",
    "# Set numpy pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "# Set the tensorflow pseudo-random generator at a fixed value\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "# Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we read train/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "X_trn, y_trn, input_shape = utils.load_data('data/sample/train.txt', 'data/sample/spc8', 'data/sample/negpeaks',\n",
    "                                            'data/sample/targets', frame_length=0.03, winfunc=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and check the shape of inputted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (240, 1)\n",
      "# of training examples: 10990\n",
      "# of samples per frame: 240\n"
     ]
    }
   ],
   "source": [
    "print('Input shape: ', input_shape)\n",
    "print('# of training examples:', X_trn.shape[0])\n",
    "print('# of samples per frame:', X_trn.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val, input_shape = utils.load_data('data/sample/val.txt', 'data/sample/spc8', 'data/sample/negpeaks',\n",
    "                                            'data/sample/targets', frame_length=0.03, winfunc=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of validation examples: 457\n"
     ]
    }
   ],
   "source": [
    "print('# of validation examples:', X_val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "In this example, we use 1D version of the InceptionV3 model which is shown in the paper to achieve best results on the test set. The definition of the model is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "model = InceptionV31D(input_shape)\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can train the model on the train set and evaluate it on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 10990 samples, validate on 457 samples\n",
      "Epoch 1/2\n",
      "10990/10990 [==============================] - 274s 25ms/step - loss: 0.1354 - accuracy: 0.9495 - val_loss: 0.4859 - val_accuracy: 0.8249\n",
      "Epoch 2/2\n",
      "10990/10990 [==============================] - 280s 25ms/step - loss: 0.1002 - accuracy: 0.9599 - val_loss: 0.6024 - val_accuracy: 0.8271\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_trn, y_trn, validation_data=(X_val, y_val), epochs=2, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "In this very simplified example, the accuracy on the validation set was about 83%. Much better results can be obtained when more training data from more voice talents is used, when tuning of the hyper-parameters (such as the frame size, batch size, learning rate, etc.) is done and also when the model is trained for more epochs. Please see the [paper](paper/Matousek_ICASSP2021_paper.pdf) for more details.\n",
    "\n",
    "Since the data is unbalanced, the _accuracy_ score could be confusing. In the [paper](paper/Matousek_ICASSP2021_paper.pdf), we use $F1$, _recall_ ($R$), and _precision_ ($P$) scores. For this purpose, we firstly get the prediction of each peak to be GCI or non_GCI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457/457 [==============================] - 4s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict to get some other metrics\n",
    "y_proba = model.predict(X_val, verbose=1)[:, 0]\n",
    "y_pred = utils.proba2classes(y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we use [Scikit-learn](http://scikit-learn.org/stable/) tools to calculate the measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 90.52%\n",
      "R  = 100.00%\n",
      "P  = 82.68%\n"
     ]
    }
   ],
   "source": [
    "print('F1 = {:.2%}'.format(skm.f1_score(y_val, y_pred)))\n",
    "print('R  = {:.2%}'.format(skm.recall_score(y_val, y_pred)))\n",
    "print('P  = {:.2%}'.format(skm.precision_score(y_val, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with the CNN model\n",
    "\n",
    "We have tuned the hyper-parameters of the InceptionV3-1D model and trained it on all data (3200 utterances) using GPU (see the [paper](paper/Matousek_ICASSP2021_paper.pdf)). The resulting pre-trained weigths are available [here](prediction/weights.h5): `prediction/weights.h5` and the model's architecture is [here](prediction/architecture.json): `prediction/architecture.json`. They could be used to set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prediction/architecture.json', 'rt') as json_file:\n",
    "    model = model_from_json(json_file.read())\n",
    "# Load optimal model's weights from hdf5 file\n",
    "model.load_weights('prediction/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction, we have to pre-process the input waveform to meet the format used when the model was trained. The following steps have to be done:\n",
    "1. The input waveform is downsampled to 8 kHz and mastered/normalized\n",
    "1. The waveform is then lowpass-filtered (with the lowpass filter coefficient stored in `prediction/filtcoef800.npy`).\n",
    "1. Negative peaks are detected using the lowpass-filtered signal.\n",
    "\n",
    "The function `preprocess` could be used to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# neg. peaks: 457\n"
     ]
    }
   ],
   "source": [
    "# Read filter coefficients\n",
    "filtcoef = np.load('prediction/filtcoef800.npy')\n",
    "# Read input waveform (16 kHz)\n",
    "samples_src, sf_src = lr.load('prediction/wav/slt_arctic_a0001.wav', sr=None)\n",
    "# Pre-process the waveform\n",
    "samples_tgt, peaks, filtsamples = utils.preprocess(samples_src, sf_src, 8000, filtcoef, norm_amp_spc=30000,\n",
    "                                                   norm_amp_filt=0.9)\n",
    "print('# neg. peaks:', len(peaks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract samples around each negative peak, this time using a 80ms frame and Hamming window, and store the input data representing it as time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = utils.frames_from_utt(samples_tgt, 8000, peaks, 0.080, np.hamming)\n",
    "X = utils.data_as_timesteps(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make the prediction. In our case, it means to assign each negative a probabilistic prediction: whether this peak represents a GCI (prediction > 0.5) or a non-GCI (prediction <= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# predictions (1/0): 457 (391/66)\n"
     ]
    }
   ],
   "source": [
    "# Predict GCIs => get a probabilistic prediction per a peak\n",
    "y = model.predict(X).flatten()\n",
    "print('# predictions (1/0): {} ({}/{})'.format(len(y), len(y[y > 0.5]), len(y[y <= 0.5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our predictions were made for 8kHz signals, we must convert them to correspond with the source sampling frequency (16 kHz in our case). We also convert the predictions from samples to time marks (in seconds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert peak indices according to SOURCE sampling frequency\n",
    "peaks_src = gu.seconds2samples(gu.samples2seconds(peaks, 8000), sf_src)\n",
    "# Get samples of the predicted GCIs synced to the nearest peak in the SOURCE signal\n",
    "pred_samps_src = gu.sync_predictions_to_samp_peak(y, peaks_src, samples_src,\n",
    "                                                  gu.seconds2samples(0.0015, sf_src),\n",
    "                                                  gu.seconds2samples(0.0020, sf_src))\n",
    "pred_times_src = gu.samples2seconds(pred_samps_src, sf_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we store the detected GCIs in the [wavesurfer](http://www.speech.kth.se/wavesurfer) format (see below) using the `Pitchmark` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pitch-marks from GCIs\n",
    "pms = gu.create_gci(pred_times_src)\n",
    "# Write GCIs to a file\n",
    "pms.write_file('prediction/gci_pred/slt_arctic_a0001.pm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the example of the detected GCIs:\n",
    "\n",
    "![GCI detection sample](figs/gci_detection_sample.png \"GCI detection sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple waveforms, the prediction can be run using the following script (please ignore the deprecation warnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING    From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING    From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING    From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "2020-11-03 16:20:17.070944: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-11-03 16:20:17.095391: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbc7ea50530 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-11-03 16:20:17.095407: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING    From /Users/jmatouse/GIT_repos/ARTIC/2021_ICASSP/.direnv/python-3.7.5/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "INFO       slt_arctic_a0001    : # GCIs = 391\n",
      "INFO       slt_arctic_a0002    : # GCIs = 340\n",
      "INFO       slt_arctic_a0003    : # GCIs = 391\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd prediction\n",
    "./detect_gci.py --architecture=architecture.json  \\\n",
    "                --weights=weights.h5              \\\n",
    "                --filt-coef=filtcoef800.npy       \\\n",
    "                --sf-tgt=8000                     \\\n",
    "                --frame-length=0.080              \\\n",
    "                --winfunc=hamming                 \\\n",
    "                --sync-left=0.0015                \\\n",
    "                --sync-right=0.0020               \\\n",
    "                './wav/*.wav'                     \\\n",
    "                './gci_pred'\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on CMU data\n",
    "\n",
    "The trained and tuned CNN model can be evaluated on the [CMU](http://festvox.org/dbs/index.html) test datasets. Again, we used the [Multi-Phase Algorithm](http://www.sciencedirect.com/science/article/pii/S0167639311000094) (MPA) to detect GCIs from the contemporaneous electroglottograph (EGG) signal and used them as the ground truth ones. The ground truth GCIs are available in the [wavesurfer](http://www.speech.kth.se/wavesurfer) format\n",
    "\n",
    "```\n",
    "0.234687 0.234687 V\n",
    "0.242312 0.242312 V\n",
    "0.250250 0.250250 V\n",
    "0.258062 0.258062 V\n",
    "0.265937 0.265937 V\n",
    "```\n",
    "\n",
    "The most important is the first column which denotes the position of a GCI in seconds. Other columns can be ignored.\n",
    "\n",
    "The ground truth GCIs for all utterances and voices can be found in the respective subfolders of the ```data/evaluation``` folder, or here:\n",
    "* [BDL](data/evaluation/bdl/bdl_gt_gci.tar.gz)\n",
    "* [SLT](data/evaluation/slt/slt_gt_gci.tar.gz)\n",
    "* [KED](data/evaluation/ked/ked_gt_gci.tar.gz)\n",
    "\n",
    "GCIs detected by different methods are stored in the ``data/evaluation/<voice>`` folder where ``<voice>`` is one of the voices we experimented with: ``bdl``, ``slt``, ``ked``.\n",
    "\n",
    "The name of the compressed file with GCIs is as follow:\n",
    "\n",
    "``<voice>_<method>_<type>_gci``\n",
    "* ``<voice>``  ...  a voice (``bdl``, ``slt``, ``ked``)\n",
    "* ``<method>`` ... GCI detection method (``dypsa``, ``mmf``, ``reaper``, ``sedreams``, ``gefba``, ``psfm``, ``xgboost``, ``inceptionV31D``)\n",
    "* ``<type>``   ... GCI type (original vs. postprocessed)\n",
    "  * ``orig`` ... original GCIs as detected by each method\n",
    "  * ``post`` ... postprocessed GCIs (V/U filtering, syncing with neighboring minimum negative sample)\n",
    "\n",
    "### Example of GCI detection evaluation\n",
    "Here is an example of the evaluation of GCI detection in terms of identification rate (IDR), miss rate (MR), false alarm rate (FAR), identification rate (IDA), and accuracy within 0.25 ms range (A25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO       slt_arctic_a0001    : IDR = 96.78%\n",
      "INFO       slt_arctic_a0002    : IDR = 93.77%\n",
      "INFO       slt_arctic_a0003    : IDR = 97.97%\n",
      "INFO       TOTAL               : IDR = 96.23%\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd prediction\n",
    "./eval_gci.py gci_gt gci_pred > eval_results.csv\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "* `gci_gt` ... directory with ground truth GCIs\n",
    "* `gci_pred` ... directory with detected (and postprocessed) GCIs\n",
    "* `eval_results.csv` ... results for each utterance from `gci_gt` and `gci_pred` directories, and total GCI detection results for all uterances.\n",
    "\n",
    "We can see that the total _identification rate_ (IDR) for our three testing waveforms was 96.23%.\n",
    "\n",
    "Any of the `<voice>_<method>_post_gci` and the corresponding ``<voice>_gti_gci`` directories (after decompressing from `data/evaluation/<voice>`) can be used to reproduce the results described in the [paper](paper/matousek_ICASSP2021_paper.pdf).\n",
    "\n",
    "For instance, the results for the BDL voice and SEDREAMS method can be obtained by calling:\n",
    "\n",
    "``eval_gci.py bdl_gt_gci bdl_sedreams_post_gci > eval_results_bdl_sedreams.csv``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
